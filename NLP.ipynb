{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "VOCAB_SIZE = 88584\n",
    "\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)\n",
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 75s 120ms/step - loss: 0.4225 - acc: 0.8046 - val_loss: 0.3470 - val_acc: 0.8662\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 74s 119ms/step - loss: 0.2390 - acc: 0.9094 - val_loss: 0.2922 - val_acc: 0.8848\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 74s 118ms/step - loss: 0.1836 - acc: 0.9319 - val_loss: 0.2865 - val_acc: 0.8968\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 76s 121ms/step - loss: 0.1509 - acc: 0.9459 - val_loss: 0.2873 - val_acc: 0.8884\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 75s 120ms/step - loss: 0.1293 - acc: 0.9547 - val_loss: 0.3123 - val_acc: 0.8848\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 74s 118ms/step - loss: 0.1145 - acc: 0.9618 - val_loss: 0.2965 - val_acc: 0.8860\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 0.0972 - acc: 0.9690 - val_loss: 0.3546 - val_acc: 0.8852\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 0.0857 - acc: 0.9725 - val_loss: 0.3448 - val_acc: 0.8686\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 0.0773 - acc: 0.9743 - val_loss: 0.3257 - val_acc: 0.8904\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 0.0684 - acc: 0.9780 - val_loss: 0.3768 - val_acc: 0.8906\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 24s 31ms/step - loss: 0.4800 - acc: 0.8567\n",
      "[0.4799555838108063, 0.856719970703125]\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 0us/step\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)\n",
    "\n",
    "#%\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "      if num != PAD:\n",
    "        text += reverse_word_index[num] + \" \"\n",
    "\n",
    "    return text[:-1]\n",
    "\n",
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93359685]\n",
      "[0.38363642]\n"
     ]
    }
   ],
   "source": [
    "def predict(text):\n",
    "  encoded_text = encode_text(text)\n",
    "  pred = np.zeros((1,250))\n",
    "  pred[0] = encoded_text\n",
    "  result = model.predict(pred)\n",
    "  print(result[0])\n",
    "\n",
    "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
    "predict(positive_review)\n",
    "\n",
    "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Play generation\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "print(text[:250])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# encode\n",
    "vocab = sorted(set(text))\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "  return np.array([char2idx[c] for c in text])\n",
    "\n",
    "def int_to_text(ints):\n",
    "  try:\n",
    "    ints = ints.numpy()\n",
    "  except:\n",
    "    pass\n",
    "  return ''.join(idx2char[ints])\n",
    "\n",
    "\n",
    "\n",
    "text_as_int = text_to_int(text)\n",
    "\n",
    "# lets look at how part of our text is encoded\n",
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", text_to_int(text[:13]))\n",
    "print(int_to_text(text_as_int[:13]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seq_length = 100  # length of sequence for a training example\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Next we can use the batch method to turn this stream of characters into batches \n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "print(sequences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_input_target(chunk):  # for the example: hello\n",
    "    input_text = chunk[:-1]  # hell\n",
    "    target_text = chunk[1:]  # ello\n",
    "    return input_text, target_text  # hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for x, y in dataset.take(2):\n",
    "  print(\"\\n\\nEXAMPLE\\n\")\n",
    "  print(\"INPUT\")\n",
    "  print(int_to_text(x))\n",
    "  print(\"\\nOUTPUT\")\n",
    "  print(int_to_text(y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create training batches\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loss function\n",
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lets examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)\n",
    "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# and finally well look at a prediction at the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "# and of course its 65 values representing the probabillity of each character occuring next"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars  # and this is what the model predicted for training sequence 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas\\.keras\\datasets\\shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n",
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "# encode\n",
    "vocab = sorted(set(text))\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "  return np.array([char2idx[c] for c in text])\n",
    "\n",
    "def int_to_text(ints):\n",
    "  try:\n",
    "    ints = ints.numpy()\n",
    "  except:\n",
    "    pass\n",
    "  return ''.join(idx2char[ints])\n",
    "\n",
    "\n",
    "\n",
    "text_as_int = text_to_int(text)\n",
    "\n",
    "# lets look at how part of our text is encoded\n",
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", text_to_int(text[:13]))\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100  # length of sequence for a training example\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (101,), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "#Next we can use the batch method to turn this stream of characters into batches \n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((100,), (100,)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):  # for the example: hello\n",
    "    input_text = chunk[:-1]  # hell\n",
    "    target_text = chunk[1:]  # ello\n",
    "    return input_text, target_text  # hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "  print(\"\\n\\nEXAMPLE\\n\")\n",
    "  print(\"INPUT\")\n",
    "  print(int_to_text(x))\n",
    "  print(\"\\nOUTPUT\")\n",
    "  print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training batches\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[-8.10732599e-03  1.28454459e-03  6.43549312e-04 ... -1.23100297e-04\n",
      "   -3.76847852e-03 -1.33665814e-03]\n",
      "  [-5.37005719e-03 -2.67456076e-03 -3.30269802e-03 ... -2.34984327e-03\n",
      "    2.04892666e-03  4.46558977e-03]\n",
      "  [-1.40979595e-03 -3.19663761e-03 -6.41988683e-03 ... -1.25414954e-04\n",
      "   -2.25309399e-04  5.39946230e-03]\n",
      "  ...\n",
      "  [-1.41652511e-03 -1.33200949e-02 -1.32215675e-03 ...  1.53856049e-03\n",
      "   -1.32704363e-03  3.11582605e-03]\n",
      "  [-3.90638132e-03 -1.21561661e-02  9.86884348e-04 ...  3.90194100e-03\n",
      "    2.18167296e-03  1.63995149e-03]\n",
      "  [-1.74674043e-03 -1.35085462e-02 -3.13620013e-03 ...  1.13303214e-03\n",
      "    5.60330879e-03  7.56025035e-03]]\n",
      "\n",
      " [[ 2.90910690e-03 -2.12667207e-03  9.55655612e-03 ... -5.07801655e-04\n",
      "   -1.04479189e-03  6.88892137e-03]\n",
      "  [ 3.46407248e-03 -7.30794203e-03  2.98870448e-03 ... -2.92648072e-03\n",
      "    5.84115647e-03  8.90666246e-03]\n",
      "  [ 8.74965824e-03 -1.59736408e-03 -3.61318281e-03 ... -3.26658343e-03\n",
      "    8.94947909e-03  7.70928455e-04]\n",
      "  ...\n",
      "  [ 1.25913182e-03 -1.32205733e-03 -6.05548080e-03 ...  8.20286293e-03\n",
      "    1.06076049e-02 -6.24073017e-03]\n",
      "  [ 6.00600243e-03 -1.15864049e-03 -1.38648693e-03 ...  9.26366821e-03\n",
      "    1.25470124e-02 -4.08911146e-03]\n",
      "  [ 5.30054374e-03 -5.53301070e-03 -5.14281821e-03 ...  4.83826734e-03\n",
      "    1.42337587e-02  3.06058652e-03]]\n",
      "\n",
      " [[-8.10732599e-03  1.28454459e-03  6.43549312e-04 ... -1.23100297e-04\n",
      "   -3.76847852e-03 -1.33665814e-03]\n",
      "  [-8.38091969e-03 -3.38513404e-04  2.26833625e-03 ...  1.92200695e-03\n",
      "   -1.73921348e-04 -2.55374308e-03]\n",
      "  [-3.30496114e-04  6.90966845e-04  3.95450555e-03 ...  4.73649567e-03\n",
      "    2.14625197e-03 -1.72060030e-03]\n",
      "  ...\n",
      "  [-5.24820387e-03 -8.05579871e-03 -6.46495842e-04 ...  5.74025512e-03\n",
      "    8.59731715e-03 -1.28033967e-03]\n",
      "  [-2.17389176e-03 -1.09404149e-02  8.53573729e-04 ...  2.09430465e-04\n",
      "    8.71371757e-03 -2.10472732e-03]\n",
      "  [-1.17347855e-03 -1.05736721e-02  1.92093628e-03 ...  1.87577587e-03\n",
      "    5.98509982e-03 -6.36217883e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 7.13012833e-03  7.65166245e-03  4.16815281e-03 ... -6.40866347e-05\n",
      "    1.62228278e-03  7.80393544e-04]\n",
      "  [ 1.02311894e-02  5.47897257e-03  4.24852269e-03 ...  3.88262910e-03\n",
      "    5.95704932e-03  3.88156826e-04]\n",
      "  [ 8.41160491e-03 -4.87069832e-04 -2.43621040e-03 ...  1.69192138e-03\n",
      "    9.37960017e-03  5.66442637e-03]\n",
      "  ...\n",
      "  [ 4.17232933e-03 -1.15064224e-02  5.47590200e-03 ...  1.33884186e-03\n",
      "   -5.62963076e-04  1.05852485e-02]\n",
      "  [ 4.05876106e-03 -1.50015997e-02  6.92478381e-04 ... -1.23360637e-03\n",
      "    4.75293119e-03  1.16235605e-02]\n",
      "  [ 5.50449034e-03 -1.71236098e-02  3.94706149e-03 ... -4.25537396e-03\n",
      "    2.95602507e-03  1.16598178e-02]]\n",
      "\n",
      " [[ 5.17909881e-03  2.76503270e-05  2.35629873e-03 ...  3.38322297e-03\n",
      "    4.19075927e-03 -4.25060047e-04]\n",
      "  [ 4.74381726e-03 -4.38237982e-03 -2.91570858e-03 ...  8.74248799e-04\n",
      "    7.73648499e-03  4.82434221e-03]\n",
      "  [-1.68739632e-03 -3.60660953e-04 -3.28357541e-03 ...  6.66195806e-03\n",
      "    1.08452886e-02 -6.19633636e-03]\n",
      "  ...\n",
      "  [-2.17746641e-03 -2.51776702e-03 -3.72037874e-03 ...  1.68305333e-03\n",
      "    8.88942974e-04  3.84334824e-03]\n",
      "  [-1.53782230e-03  2.63172784e-04 -4.22788132e-03 ...  1.24915689e-03\n",
      "    1.48781436e-03  2.03260873e-03]\n",
      "  [-8.87672603e-03  2.28747330e-03 -3.63070192e-03 ...  2.03695078e-03\n",
      "   -3.70182283e-03  1.07930554e-03]]\n",
      "\n",
      " [[ 2.74042995e-03 -3.61186988e-03  2.21819687e-03 ... -3.28138354e-03\n",
      "   -4.57523012e-04  4.72196285e-03]\n",
      "  [-1.24666071e-03 -2.38326984e-03  2.62640789e-03 ...  1.67773804e-03\n",
      "   -4.01441567e-03  3.52251064e-03]\n",
      "  [-1.46596925e-04 -5.58460597e-03 -1.83959049e-03 ... -8.99560517e-04\n",
      "    3.90561507e-03  8.01604893e-03]\n",
      "  ...\n",
      "  [-6.28274167e-03 -7.65597331e-04  1.04662147e-03 ...  9.50533152e-03\n",
      "    1.05420640e-02 -5.63438842e-03]\n",
      "  [-6.74375705e-03 -2.92868353e-03  4.14913381e-03 ...  8.22573062e-03\n",
      "    1.22239403e-02 -5.29260794e-03]\n",
      "  [-9.80554614e-04 -4.62654233e-03  1.39883738e-02 ...  4.81447717e-03\n",
      "    7.16916565e-03  3.68203642e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[-0.00810733  0.00128454  0.00064355 ... -0.0001231  -0.00376848\n",
      "  -0.00133666]\n",
      " [-0.00537006 -0.00267456 -0.0033027  ... -0.00234984  0.00204893\n",
      "   0.00446559]\n",
      " [-0.0014098  -0.00319664 -0.00641989 ... -0.00012541 -0.00022531\n",
      "   0.00539946]\n",
      " ...\n",
      " [-0.00141653 -0.01332009 -0.00132216 ...  0.00153856 -0.00132704\n",
      "   0.00311583]\n",
      " [-0.00390638 -0.01215617  0.00098688 ...  0.00390194  0.00218167\n",
      "   0.00163995]\n",
      " [-0.00174674 -0.01350855 -0.0031362  ...  0.00113303  0.00560331\n",
      "   0.00756025]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# lets examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)\n",
    "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[-8.1073260e-03  1.2845446e-03  6.4354931e-04  3.9793514e-03\n",
      " -2.8175437e-03 -3.1024881e-03 -1.6372800e-03  4.7764834e-04\n",
      "  3.3297979e-03 -3.4942443e-04 -2.2513201e-03  1.6056071e-04\n",
      "  1.8885987e-03 -4.0045548e-03 -6.5391522e-04 -4.1217529e-03\n",
      " -4.2110342e-03 -4.4678617e-03  3.2384628e-03  2.3067431e-03\n",
      "  5.6732334e-03 -8.6032040e-04 -1.3159506e-03 -3.9123208e-03\n",
      "  2.1334905e-03 -3.5581626e-03  2.4828082e-03  1.2455152e-03\n",
      " -2.0108838e-04 -1.8162437e-03 -1.6570918e-03  2.8189551e-03\n",
      "  1.1046596e-03  5.2460814e-03  6.3025663e-03 -8.3947438e-05\n",
      "  6.5005547e-04  2.2041542e-03 -3.0050101e-03 -5.6278007e-04\n",
      "  1.4905022e-03  5.3270918e-04 -1.3152547e-03 -4.2042714e-03\n",
      " -6.3619977e-03 -5.5820053e-03  9.0459367e-04  3.2273808e-04\n",
      "  2.8965631e-03 -1.1569010e-03  9.4950451e-03  2.9212437e-03\n",
      "  2.6008049e-03 -2.0516026e-03 -7.8662415e-04  6.1713043e-05\n",
      "  1.3328278e-03 -4.6540946e-03 -2.7024797e-03  3.6181430e-03\n",
      "  2.7597288e-03  1.3742589e-03 -1.2310030e-04 -3.7684785e-03\n",
      " -1.3366581e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# and finally well look at a prediction at the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "# and of course its 65 values representing the probabillity of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gfks;PfTAq.Our?Bk!?dBwruzEnz:tes$gi,D 3c$TWs,aMYCxQtMODFsBdQU,,,\\nBWuqw:fkoxe$r.vzBJwHtMOh&LkGR\\nYk? d'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars  # and this is what the model predicted for training sequence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "172/172 [==============================] - 1173s 7s/step - loss: 2.5545\n",
      "Epoch 2/50\n",
      "125/172 [====================>.........] - ETA: 12:07 - loss: 1.9270"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}